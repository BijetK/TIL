# 0516 

# 0. 작업 진행 상황

1. ~~파이썬 기초~~
2. ~~데이터 크롤링~~
3. 전처리 기초
    1) Python vs SQL
    2) 형태소
    3) 모델 데이터셋 생성
4. ~~시각화~~
5. 비지도학습
    1. ~~연관성 분석~~
    2.  무언가
6. 지도학습 - 기초

---



# 1. 파이썬 기초

## 1. 데이터 유형

- set : 값이 중복되지 않음 `{1,2,3,4,5}`, add 사용
- list : 중복을 허용함 `[1, 2, 3, 4, 5, 1]`, append 사용
- dict : 키는 중복 허용 x, 밸류는 중복 허용 `{'key:1':6, 'key:2':2, 'key:3':3, 'key:4':4, 'key:5':5}`
- series : 데이터프레임의 부분집합

## 2.조건문

- 문자열은 각 문자를 ascii 숫자로 변환 후 제일 앞 문자부터 차례차례 비교,

## 3. Database

- 연결 

    ```python 
    conn = sqlite3.connect(':memory:')
    curs = conn.cursor
    ```

- 테이블 생성

    ```python
    curs.execute('''
    	create table sample
    	c1		int,
    	c2		float
    	c3		text
    	)
    	''')
    conn.commit()
    ```

- 데이터 입력

    ```python
    df.to_sql('sample', conn, if_exists='append', index = None)
    ```

- 데이터 읽기

    ```python
    curs.execute('select * from sample')
    rs = curs.fetchall()
    for row in rs:
        print('c1:[',row[0], '], c2:[:', row[1], '], c3:['row[2],']')
    ```

## 4. 병렬처리

- multiprocessing.Pool()

    ```python
    list_param = list()
    for idx in range(10):
        dict_param = dict()
        dicr_param['index'] = idx
        list_param.append(dict_param)
        
    p = multiprocessopnf.Pool(3)
    p,map(parallel_print, list_param)

---



# 2. 데이터 크롤링

## 1. Urllib

- [`urllib.request`](https://docs.python.org/ko/3/library/urllib.request.html#module-urllib.request) :  URL을 열기 위한 확장 가능한 라이브러리
    - urllib.request.urlopen : 문자열이나 request 객체일 수 있는 url을 연다.
    - urllib.request.Request : url 요청의 추상화, url은 유효한 url을 포함한 문자여야 한다.
- [`urllib.parse`](https://docs.python.org/ko/3/library/urllib.parse.html#module-urllib.parse)  : URL을 구성요소로 구문 분석

## 2. Requests

- requests.get(url_full_path, headers = headers)
    - ㅇ
- **urllib.request : 뒤에 s 안붙음    vs    resuqests.get : 뒤에 s 붙음 주의!!!!**
- 

## 3. Selenium 



## 4. 공통 로그 함수 (line_logging)

- log_time = datetime.datetime.today().strftime('[%Y/%m/%d %H:%M:%S]'

    : datetime.datetime.today() : 2022-05-16 09:14:30 이런 형식

    : strftime() : 날짜 형식을 지정해주는 메서드 (**STR**ing **F**ormat **TIME**)

    	- 실행 결과 : [2022/05/16 09:36:25]:[https://finance.naver.com/sise/sise_index_day.nhn?code=KOSPI&page=1] 
    	- 실행 결과 : [2022/05/16 09:36:27]:[save_kospi_and_kosdaq KOSPI Start: 20220509 , Finish:  20220516 0 (6, 6)]

## 5. Crawling Func

- return값 : resluts = requests.get(url_full_path, headers = headers)
- p_flag_view_url = True : 매개변수의 초깃값 설정, 매개변수가 항상 변하는 것이 아닐 때 유용.
- 





## 6. HTML Parsing(코스닥/코스피)

- beutifulsoap(results.content, "html.parser").find_all('table')[0]

    : 'table' 조건에 해당되는 모든 태그를 list로 리턴

- 수집결과 저장

    1. df + df_page, sort = False
    2.  drop.duplicates() : 중복 제거
    3. set_index(['eod_date']) : eod 인덱스 설정
    4. sort.index(ascending = False) : 내림차수 정렬
    5. reset_index() : 인덱스 리셋

- eod_date : 그냥 날짜....



## 7. JSON Loading (한국은행)

- req = urllib.request.Request(base_url + urllib.parse.unquote(params), headers = headers)
- response_body = url.parse.urlopen(req).read()
- json_data = json.loads(response_body)

## 8. Selenium + 네이버 로그인

- 클립보드에 input 복사 후, auto chain을 이용해 로그인 폼에 붙여넣기

- 네이버 로그인 코드

    ```python
    options = selenium.webdriver.ChromeOptions()
    options.add_experimental_option("excludeSwitches", ["enable-logging"])
    ```

    ```python
    driver = selenium.webdriver.Chrome(driver_full_path, options = options)
    ```

## 9. API (기상청)



## 10. enuerate

- 인자로 넘어온 목록을 기준으로 인덱스와 원소를 차례대로 접근하게 해주는 반복자(iterator) 객체를 반환해주는 함수

---



# 3. 전처리 기초

## 1. Python vs SQL

### 0. SQL 연결 및 기초

-   연결

    ```python
    db_conn = sqlite3.connect(':memory:')
    t1.to_sql('t1', db_conn, if_exists='replace')
    ```

-   끝

    ```python
    df_from_db = pd.read_sql(sql_select, db_conn)
    ```

    

### 1. UNION (수직결합 + 중복은 제거)

-   **concat**([df1, df2], sort=False)

    df = df.drop_duplicates()

    

-   **select** c1, c2, c3

    **from** t1

    **union**

    **select** c1, c2, c3

    **from** t2

### 2. UNION ALL (중복 허용)

-   **concat**([df1, df2], sort=False)

    

-   **select** c1, c2, c3

    **from** t1

    **union all**

    **select** c1, c2, c3

    **from** t2



### 3. RELATIVE COMPLEMENT (차집합)

-   ```python
    set_t2 = set(t2['c1'].tolist())
    df = t1[~t1['c1'].isin(set_t2)]
    print(df.shape)
    display(df)
    ```

-   select c1, c2, c3

    from t1

    where not exists (**SELECT** 1 **FROM** t2 **WHERE** t1.c1 = t2.c1 )

### 4. INNER JOIN

-   1.   공통열 인덱스화
    2.   df_t1.join( df_t2, how = 'inner', lsuffix = '', rsuffix = '_DEL' )
    3.   reset_index()
    4.   필요한 열만 추출

    

-     **select** t1.c1, t1.c2, t1.c3, t2.c4

      **from** t1

      **inner join** t2

      **on** t1.c1 = t2.c1

    

### 5. LEFT OUTER JOIN

-   1.   공통열 인덱스화

    2.   df_t1.join( df_t2, how = 'left, lsuffix = '', rsuffix = '_DEL' )

    3.   reset_index()

    4.   필요한 열만 추출

         

-   **select** t1.c1, t1.c2, t1.c3, t2.c4

    **from** t1

    **left outer join** t2

    **on** t1.c1 = t2.c1



### 6. GROUP BY

-   df = t1.groupby(['c1', 'c2']).agg( cnt_c3 = ('c3', 'count')).reset_index()

    

-   **select** c1, c2, `count(c3) as cnt_c3`

    **from** t1

    **group by** c1, c2



### 7. WHERE

-   df = t2 [ t2 [ 'c4' ] == 22111 ]



-   **select** c1, c2, c3

    **from** t2

    **where** c4 = 22111 



### 8. HAVING

-   ```python
    df = df[df['cnt_c3'] > 1311]
    ```

    

-   **select** c1, c2, sum(c3) as cnt_c3

    **from** t1

    **group by** c1, c2

    **having** sum(c3) > 1311 



## 2. 형태소

### 1. 패키지

-   PyKomoran : JAVA의 형태소 분석기인 Komoran을 파이썬에서 사용할 수 있게 한 것

### 2. 



## 3. 전처리를 통한 모델 데이터셋 생성

### 1. 결측치 처리 및 모델링 변수 활용 방법

-   범주형 (명목형 또는 순서형) : 각 조건별로 One-Hot Encoding

-   연속형 (수치형) - 구간 설정 후 각 구간별로 One-Hot Encoding (EDA를 통한 구간 정의 필요)

    ​							  또는 연속 데이터로 사용

-   Shift 및 Rolling울 활용한 시계열 데이터 생성



### 2. 필요한 데이터 유지(필요없는 데이터 삭제)

-   ```python
    df.drop('year', axis = 1, inlpace = True)
    ```

-   ```python
    columns = [ ~~~~~ ]
    df = df.columns
    ```



### 3. 결측치 삭제

-   ```python
    # sales 변수 기준으로 결측치 행 삭제
    
    ## 방법 1 : 결측치를 제외한 인덱스 유지
    sales_df = saled_df[sales_df['sales'].isna() != True].reset_index(drop=True)
    
    ## 방법 2 : 결측치 인덱스를 드롭한 남은 인덱스 추출
    salesd_df = sales_df.iloc[sales_df['sales'].dropna().index()].reset_index(drop=True)
    ```



### 4. 결측치 채우기 (문자열 > '')

-   **fillna**('', inplace = True)

    ```python
    sales_df.promo_bin_1.fillna('', inplace = True)
    ```



### 5. 가격 결측치 채우기

-   1.   가격정보 유무에 따라 결측치 데이터 분리 (df1, df0)
    2.   가격정보 없는 매장의 상품 정보가 가격정보가 있는 데이터에서 존재하는지 확인
         1.   상품코드 / 매장코드 기준으로 df1, df0 merge (how = 'inner')       **dfz**
         2.   상품코드 / 매장코드 / 날짜 / 판매가격 순으로 정렬 (ascending = [True,True,True,True])
         3.   df0 에서 price column 제거 후 dfz와 merge
    3.   merge 후에도 가격정보 있는지 없는지 확인
         1.   가격정보 유무에 따라 df00, df01로 구분
         2.   df00에서 매장구분 없이 같은 가격정보를 가져오려 함
         3.   상품의 최소가격을 기준으로 그룹핑
         4.   df00에서 price 칼럼 제거 후 dfz0와 merge
    4.   df1, df0, df00 concat



### 6. 원핫인코딩

-   가격 결측치 업데이트 이후에도 가격정보가 없을 경우, 가격정보 유무 여부 변수로 구분자 생성

    ```python
    sales_df_fixed.loc[sales_df_fixed['price'].isna()==True, 'price_ohe'] = 1
    sales_df_fixed.loc[sales_df_fixed['price'].isna()==False, 'price_ohe'] = 0
    sales_df_fixed['price_ohe'] = sales_df_fixed['price_ohe'].astype(int)
    ```

-   Ex) promo_type_1 유형별 원핫인코딩

    ```python
    for i in sales_df_fixed.promo_type_1.unique().tolist():
        if i == '':
            continue
        else:
            # 변수 생성
            ## promo_type_1_000의 기본값을 0으로 설정
            sales_df_fixed['promo_type_1_'+ i] = 0
            ## promotype1이 i인 새 칼럼의 값을 1로 변경
            sales_df_fixed.loc[sales_df_fixed.promo_type_1 == i, 'promo_type_1_'+ i] == 1
            
    # promo_type_1 변수 삭제
    sales_df_fixed.drop('promo_type_1', axis = 1, inplace=True)
    ```

    => for문을 사용해서 각 변수마다 실행시켜준 후 원핫인코딩 완료된 변수 삭제



### 7. 그룹핑

-   product_id 기준으로,

-   store_id 기준으로,

-   product_id, store_id 기준으로

-   매장별 상품별 판매수량 (pivot table)

    ```python
    store_product_pivot = sales_df_fixed.pivot_table(index = 'product_id', columns = 'store_id', values='sales').fillna(0)
    ```

    ![pp_pivot_table](C:\workspace\Typora\images\pp_pivot_table.png)

    

---



# 4. 시각화

## 1. groupby( ).agg( )

- DF.groupby([컬럼1,컬럼2]).agg({계산하고싶은컬럼명1 : 집계함수 , 계산하고싶은컬럼명2 : 집계함수})

## 2. [Pie Chart](https://wikidocs.net/92114) (`plt.pie( )`)

```python
plt.pie(list_Data, labes=lidt_labels, autopct = '%.0f%%')
```

- plt.pie( 영역 비율, 영역 이름, 표시될 숫자의 형식 )![pie_chart](C:\workspace\Typora\images\pie_chart.png)

## 3. [Bar Chart, Column Chart](https://hleecaster.com/python-seaborn-barplot/)(`sns.barplot( )`)

```python 
as = sns.barplot(x= list_date, y = list_labels)
```

- sns.barplot(데이터프레임, 어떤 열을 레이블로 지정할지, 어떤 열을 값으로 지정할 지)

- x축이 라벨, y축이 수치면 수직 막대, x축이 수치, y축이 라벨이면 수평 막대 

![bar_chart](C:\workspace\Typora\images\bar_chart.png)

![column_chart](C:\workspace\Typora\images\column_chart.png)

## 4. Line Chart (`sns.lineplot( )`)

```python 
ax = sns.lineplot(y = list_data, x = list_labels)
```

![line_chart](C:\workspace\Typora\images\line_chart.png)

## 5. Area Chart (`plt.stackplot( )`)

```python 
plt.stackplot(df_transpose.index, df_transpose['PV-Piece-90'], df_transpose['PV-Piece-80'], df_transpose['PV-Piece-70'], df_transpose['PV-Piece-60'])
```

![area_chart](C:\workspace\Typora\images\area_chart.png)

## 6. Scatter Chart(`sns.scatterplot( )`)

```py
sns.scatterplot(x=df_transpose.index, y= df_transpose['PV-Piece-90'], s = 500)
```

![scatter_chart](C:\workspace\Typora\images\scatter_chart-16526670257272.png)

## 7. Secondary Axis ( )

![secondary_axis](C:\workspace\Typora\images\secondary_axis.png)

## 8. Heatmap Chart (`sns.heatmap( )`)

```python
sns.heatmap(df_chart, annot=False, cmap='coolwarm', robust = True,\
            fmt = '.0f', linewidth = .5)
```

![heatmap](C:\workspace\Typora\images\heatmap.png)

- heatmap 문법

    ``` python
    sns.heatmap(df, vmin = 100, vamx = 700, cbar = True, center=400, linewidth = 0.5, annot=True, fmt = d, cmap = 'Blues')
    ```

    - df : 데이터
    - vmin : 최솟값
    - cmax : 최대값
    - cbar : color바의 유무
    - center : 중앙값
    - linewidths : cell 사이에 선을 집어넣음
    - annot, fmt : 각 cell의 값 표기 유무, 그 값의 데이터 타입
    - cmap : 히트맵의 색을 설정

---



# 5. **비지도학습**

## 1. 연관성 분석

### 1. 패키지

- mlxtend.frequent_patterns : Machine Learning Extension
- yellowbrick.cluster : 머신러닝 시각화 도구
- sklearn.cluster
- sklearn.metrics
- sklearn.decompostion

### 2. 데이터 전처리

- np.where( 배열에 대한 조건문,  참일때 값, 거짓일때 값 ) , array의 형태로 반환

- sort_value( by ='열이름',  ascending = True ) / 내림차순의 경우 ascending = False

- strip( ) : 양 끝 공백을 제거

- pd.pivot_table( ) : 피벗테이블 생성

    ```python
    pdf1 = pd.pivot_table(df,                # 피벗할 데이터프레임
                         index = 'class',    # 행 위치에 들어갈 열
                         columns = 'sex',    # 열 위치에 들어갈 열
                         values = 'age',     # 데이터로 사용할 열
                         aggfunc = 'mean')   # 데이터 집계함수
    ```

- 

### 3. 연관성분석 (Association Rule) 실행

- 연관 규칙 분석 : 어떤 두 아이템 집합이 번번히 발생하는가를 알려주는 일련의 규칙을 생성하는 알고리즘

    ```python
    mlxtend.frequent_patterns.apriori(df_ar, min_support=dict_args['min_support'], use_colnames = True, verbose = 1)
    ```

    ```python
    df_rules = mlxtend.frequent_patterns.association_rules(freq_items, metric= 'confidence', min_threshold = dict_args['min_threshold']).sort_values(by = ['lift', 'confidence', 'support'], ascending = False)
    ```

### 4. 연관성 분석 결과 확인

![AssociationRulesResult](C:\workspace\Typora\images\AssociationRulesResult.png)

## 2. 군집 분석

### 1. 실루엣분석

- **실루엣 분석 (Silhouette Analysis)** : 각 군집 간의 거리가 얼마나 효율적으로 분리 돼있는지를 나타냄,

- **실루엣 계수 (Silhouette Coeff)** : <u>각 데이터 포인트</u>와 주위 데이터 포인트들과의 거리 계산을 통해 값을 구하며, 군집 안에 있는 데이터들은 잘 모여있는지, 군집끼리는 서로 잘 구분되는지 클러스터링을 평가하는 척도로 활용된다. 실루엣 계수의 평균값이 1에 가까울수록 군집화가 잘 되었다고 생각할 수 있다.

    -   계수가 0에 가까움 : 두 군집이 잘 구분되지 않음

    -   계수가 -1에 가까움 : 데이터포인트 i의 군집이 잘못 설정되었음.

- 장점 : 클러스트링 수행 후에 분석 > 클러스터링 알고리즘에 영향X

      		 		 적절한 클러스터 개수 or 더 나은 클러스터링 기법 선택 가능

    ​		  결과값 시각화 가능

- 단점 : 데이터 양이 많을수록 오래걸린다.

    ​		   전체 평균만으로 클러스터링 결과를 판단할수 없음, 개별 클러스터의 평균값도 함께 고려 필요

### 2. 데이터 전처리

- ['CustomerID']  :  fillna(-1) > astype(float) > astype(int) > astype(str)
- ['Amount']  :  ['UnitPrice'] * ['Quantity']

### 3. 군집분석 실행

- 

### 9. [사이킷런 홈페이지 예시](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

---



# 6. 지도학습

## 1. 층화 추출

- 층화 표본 추출법

    : 한 모집단을 동질적인 소집단들로 층화시킨 후 그 집단의 크기에 따라 단순 무작위 표본 추출방법을 사용하여 표본을 추출

    : 범주형 데이터에 적용하는 기법,

## 9. 함수관련

### build_dataset(p_args)

-   if 절 순서
    1.   col_name == check_col_name
    2.   'GROUP-SQUARE-MOVE-'
    3.   'GROUP-SQUARE-'
    4.   'GROUP-ROOT-'
    5.   'GROUP-LOG-'
    6.   'GROUP-MOVE-'
    7.   'GROUP-'
    8.   'SQUARE-MOVE-'
    9.   'SQUARE-'
    10.   'ROOT-'
    11.   'LOG-'
    12.   'MOVE-'

-  '~SQUARE-'  들어갈 경우 

    ```python
    df_part['SQUARE-' + col_name] = df_part[col_name] * df_part[col_name]
    ```

    들어감

- 'MOVE- ' 들어갈 경우 

    ```python
    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1
    ```

    들어감

- 'ROOT-' or 'LOG-' 들어감

    ```python
    if min_value <=0 들어감
    
    # GROUP 포함일 떄,
    if:
        df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1
        df_part = np.sqrt or np.log
    else:
        df_part = np.sqrt or np.log
        
    # GROUP 미포함일때
    if:
        df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1
        df_target = np.sqrt or np.log
    else:
        df_target = np.sqrt or np.log
    ```

- randomforestClassifer

    -   max_Depth = 
    -   min_sample_split = 
    -   min_sample_leaf = 
    -   random_state = 
    -   n_estimator = 

# +a 추가 필요

- K-Means (군집분석에서 사용, 개념 다시 확실하게 알아 둘 필요가 있음)
    -   중심기반 비계층적 군집분석
    -   유사한 데이터는 중심점(centroid)를 시반으로 분포할 것이라는 가정을 기반
    -   n개의 데이터와 k개의 중심점이 주어졌을때, 각 그룹내의 데이터와 중심점 간의 비용(거리)을 최소화하는 방향으로 업데이트를 진행함으로써 그룹화를 하는 기법
    -   sklearn.cluster.KMeans(n_cluster=, init=, random_state=)
        -   n_cluster : k의 수
        -   init : 초기값 지정(디폴트는 random), 'k-means++' : [k-means++ 알고리즘](https://itstory1592.tistory.com/19) 사용
        -   random_state : random 시드 설정.
    -   

-   
-   Information Value가 무엇인지

-   sqrt = SQuare RooT
-   